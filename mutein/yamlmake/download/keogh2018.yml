#define dataset_id and subset_list
- include: "../config/keogh2018.yml"

#download list of accession names
- action:
    name:  "download_accession_list"
    exec:  "local"
    conda: "entrez"

    #no input files

    output:
        accession_file: "datasets/{%dataset_id}/{=subset_id}/.meta/accession_list"

    #grab list of sra run accessions from NCBI (US site)
    shell: |
        esearch -db sra -query {=subset_id} \
        | efetch -format xml \
        | xtract -pattern RUN -element PRIMARY_ID \
        | awk '{{print $1}}' \
        | sort \
        > {%accession_file}

#download ftp link and md5 for each datafile using ena portal api
- action:
    name: "download_metadata"
    exec:  "local"

    url_prefix: "https://www.ebi.ac.uk/ena/portal/api/filereport?accession="
    url_suffix: "&result=read_run&fields=run_accession,fastq_ftp,fastq_md5"

    input:
        #spawn one job per subset_id, currently there's only one for keogh
        accession_list: "datasets/{%dataset_id}/{=subset_id}/.meta/accession_list"

    output:
        touch: "datasets/{%dataset_id}/{=subset_id}/.meta/{%name}.touch"

    shell: |
        for acc in $(cat {%accession_list})
        do
            mkdir -p datasets/{%dataset_id}/{=subset_id}/${acc}/.meta
            outfile="datasets/{%dataset_id}/{=subset_id}/${acc}/.meta/link_and_md5"
            query='{%url_prefix}'${acc}'{%url_suffix}'
            wget -O "${outfile}" "${query}"
        done
        touch {%touch}

#wget download the actual fastq.gz files
- action:
    name:  "download_accessions"
    exec:  "local"

    input:
        #avoid using dir as the input as its mtime updates when any of its contents change
        metadata: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/link_and_md5"

    output:
        read1: "datasets/{%dataset_id}/{=subset_id}/{*accession}/{*accession}_1.fastq.gz"
        read2: "datasets/{%dataset_id}/{=subset_id}/{*accession}/{*accession}_2.fastq.gz"
        #note: most also have {*accession}.fastq.gz as well

    shell: |
        for url in $(tail -n1 {%metadata} | awk '{print $2}' | tr ';' ' ')
        do
            wget --continue --directory-prefix=datasets/{%dataset_id}/{=subset_id}/{*accession} "ftp://${url}"
        done

#check fastq.gz files
- action:
    name:  "verify_downloads"
    exec:  "qsub"
    conda: "main"

    qsub:
        time:  "1:00:00"
        mem:   "2G"
        tmpfs: "2G"
        cores: "2"

    input:
        metadata: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/link_and_md5"
        read1: "datasets/{%dataset_id}/{=subset_id}/{*accession}/{*accession}_1.fastq.gz"

    output:
        touch: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/md5_okay"

    shell: |
        fnames=( $(tail -n1 {%metadata} | awk '{print $2}' | tr ';' ' ') )
        md5s=( $(tail -n1 {%metadata} | awk '{print $3}' | tr ';' ' ') )

        for (( i=0; i<${#fnames[*]}; ++i))
        do
            filename='datasets/{%dataset_id}/{=subset_id}/{*accession}/'$(basename ${fnames[$i]})
            obs=$(md5sum ${filename} | awk '{print $1}')
            exp=${md5s[$i]}
            if [ ${obs} != ${exp} ]
            then
                echo "MD5 mismatch: ${fnames[$i]} expecting ${exp} but found ${obs}"
                exit 1
            fi
        done

        touch {%touch}

#copy manually downloaded metadata file into working position
#see download/setup.yml
- action:
    name: "move_meta_data"
    exec: "local"

    input:
        src:  "manual_downloads/{%dataset_id}/SraRunTable.txt"

    output:
        dst:  "datasets/{%dataset_id}/{=subset_id}/.meta/SraRunTable.txt"

    shell: |
        cp {%src} {%dst}

#extract individual information from metadata for each accession
- action:
    name:  "extract_indiv_info"
    exec:  "local"
    conda: "main"

    input:
        metadata:        "datasets/{%dataset_id}/{=subset_id}/.meta/SraRunTable.txt"
        glob_accessions: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/link_and_md5"

    output:
        acc_meta:  "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/accession_metadata"
        sample_no: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/sample_number"
        all_acc_list:  "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/all_sample_accessions"
        other_acc_list:  "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/other_sample_accessions"

    shell: |
        #extract accession's metadata line from metadata file
        tail -n +2 {%metadata} | grep -w -e '^{*accession}' > {%acc_meta}

        #extract sample number (at csv column 22 as SAMPLENUMBER_XXX_XXX
        cat {%acc_meta} | mutein cut -f 22 | cut -d_ -f1 > {%sample_no}
        sample=$(cat {%sample_no})

        #find all accessions with the same sample number
        tail -n +2 {%metadata} | mutein cut -f 1 22 -o '_' | tr '_' ' ' | cut -d' ' -f1,2 \
        | grep -w -e "${sample}" \
        | awk '{print $1}' \
        > {%all_acc_list}

        #exclude the current accession from the list
        set +o pipefail
        cat {%all_acc_list} \
        | grep -wv -e '{*accession}' | cat \
        > {%other_acc_list}

#create per-individual information
- action:
    name:  "per_individual_info"
    exec:  "local"
    conda: "main"

    input:
        metadata: "datasets/{%dataset_id}/{=subset_id}/.meta/SraRunTable.txt"

    output:
        indiv_list: "datasets/{%dataset_id}/{=subset_id}/.meta/indiv_list"
        acc2indiv:  "datasets/{%dataset_id}/{=subset_id}/.meta/acc2indiv"
        touch:      "datasets/{%dataset_id}/{=subset_id}/.meta/{%name}.touch"

    shell: |
        #extract list of indiv numbers (at csv column 22 as SAMPLENUMBER_XXX_XXX
        tail -n +2 {%metadata} | mutein cut -f 22 | cut -d_ -f1 | sort -u > {%indiv_list}

        #create file containing only accession and individual
        tail -n +2 {%metadata} | mutein cut -f 1 22 | tr ',_' ' ' | cut -d' ' -f1,2 > {%acc2indiv}

        #create list of accessions from each individual
        for indiv in $(cat {%indiv_list})
        do
            mkdir -p datasets/{%dataset_id}/{=subset_id}/.meta/indivs/${indiv}
            cat {%acc2indiv} | grep -w -e ${indiv} | cut -d' ' -f1 \
                > datasets/{%dataset_id}/{=subset_id}/.meta/indivs/${indiv}/accession_list
        done

        touch {%touch}

#create intervals.list file specifying the position of all the target capture genes
#for the keogh2018 dataset
- action:
    name:  "create_intervals_list"
    exec:  "local"

    input:
        mart_export_gz: "manual_downloads/keogh2018/mart_export.txt.gz"
        gene_list:      "{$MUT_DIR}/mutein/fixtures/keogh2018_gene_list"

    output:
        all_genes:  "datasets/{%dataset_id}/.meta/all_genes.tsv"
        uniq_genes: "datasets/{%dataset_id}/.meta/uniq_genes.tsv"
        gene_posns: "datasets/{%dataset_id}/.meta/gene_positions.tsv"
        intervals:  "{%gene_intervals_file}"

    shell: |
        #uncompress and rename
        zcat {%mart_export_gz} > {%all_genes}

        #make a version of the list lacking the synonyms column
        cat {%all_genes} \
            | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6}' \
            | sort -u \
            > {%uniq_genes}

        #extract gene positions for keogh2018 genes only
        #where gene_list contains the 102 gene names from supplementary table 1
        #note: NR is total lines processed, FNR is lines processed in current file
        #therefore NR == FNR for the first file only
        awk 'NR == FNR {genes[$0]} NR > FNR && ($6 in genes)' {%gene_list} {%uniq_genes} \
            | grep -v CHR_ \
            | sort -u \
            > {%gene_posns}

        #print only the position and full chromosome name in the required format
        awk '{print "chr"$3":"$4"-"$5}' {%gene_posns} \
            > {%gene_intervals_file}

