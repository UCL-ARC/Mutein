#define dataset_id and subset_list
- include: "../config/keogh2018.yml"

#download list of accession names
- action:
    name:  "download_accession_list"
    exec:  "local"
    conda: "enabrowsertools"

    #no input files

    output:
        accession_file: "datasets/{%dataset_id}/{=subset_id}/.meta/accession_list"

    #grab list of sra run accessions from NCBI (US site)
    shell: |
        esearch -db sra -query {=subset_id} \
        | efetch -format xml \
        | xtract -pattern RUN -element PRIMARY_ID \
        | awk '{{print $1}}' \
        | sort \
        > {%accession_file}

#download ftp link and md5 for each datafile using ena portal api
- action:
    name: "download_metadata"
    exec:  "local"

    url_prefix: "https://www.ebi.ac.uk/ena/portal/api/filereport?accession="
    url_suffix: "&result=read_run&fields=run_accession,fastq_ftp,fastq_md5"

    input:
        #spawn one job per subset_id, currently there's only one for keogh
        accession_list: "datasets/{%dataset_id}/{=subset_id}/.meta/accession_list"

    output:
        touch: "datasets/{%dataset_id}/{=subset_id}/.meta/{%name}.touch"

    shell: |
        for acc in $(cat {%accession_list})
        do
            mkdir -p datasets/{%dataset_id}/{=subset_id}/${acc}/.meta
            outfile="datasets/{%dataset_id}/{=subset_id}/${acc}/.meta/link_and_md5"
            query='{%url_prefix}'${acc}'{%url_suffix}'
            wget -O "${outfile}" "${query}"
        done
        touch {%touch}

#wget download the actual fastq.gz files
- action:
    name:  "download_accessions"
    exec:  "local"

    input:
        #avoid using dir as the input as its mtime updates when any of its contents change
        metadata: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/link_and_md5"

    output:
        read1: "datasets/{%dataset_id}/{=subset_id}/{*accession}/{*accession}_1.fastq.gz"
        read2: "datasets/{%dataset_id}/{=subset_id}/{*accession}/{*accession}_2.fastq.gz"
        #note: most also have {*accession}.fastq.gz as well

    shell: |
        for url in $(tail -n1 {%metadata} | awk '{print $2}' | tr ';' ' ')
        do
            wget --continue --directory-prefix=datasets/{%dataset_id}/{=subset_id}/{*accession} "ftp://${url}"
        done

#make sure user has downloaded the metadata file from:
#https://www.ncbi.nlm.nih.gov/sra/?term=SRP159015
# ==> "Send results to run selector"
# ==> "Select" ==> "Total" ==> "Metadata"
- action:
    name: "require_meta_data"
    exec: "local"

    input:
        metadata: "datasets/{%dataset_id}/{=subset_id}/.meta/SraRunTable.txt"

    output:
        touch:    "datasets/{%dataset_id}/{=subset_id}/.meta/{%name}.touch"

    shell: |
        touch {%touch}

#extract individual information from metadata for each accession
- action:
    name:  "extract_indiv_info"
    exec:  "local"
    conda: "main"

    input:
        metadata: "datasets/{%dataset_id}/{=subset_id}/.meta/SraRunTable.txt"
        download_trigger: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/download_pending"

    output:
        acc_meta:  "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/accession_metadata"
        sample_no: "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/sample_number"
        all_acc_list:  "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/all_sample_accessions"
        other_acc_list:  "datasets/{%dataset_id}/{=subset_id}/{*accession}/.meta/other_sample_accessions"

    shell: |
        #extract accession's metadata line from metadata file
        tail -n +2 {%metadata} | grep -w -e '^{*accession}' > {%acc_meta}

        #extract sample number (at csv column 22 as SAMPLENUMBER_XXX_XXX
        cat {%acc_meta} | mutein cut -f 22 | cut -d_ -f1 > {%sample_no}
        sample=$(cat {%sample_no})

        #find all accessions with the same sample number
        tail -n +2 {%metadata} | mutein cut -f 1 22 -o '_' | tr '_' ' ' | cut -d' ' -f1,2 \
        | grep -w -e "${sample}" \
        | awk '{print $1}' \
        > {%all_acc_list}

        #exclude the current accession from the list
        set +o pipefail
        cat {%all_acc_list} \
        | grep -wv -e '{*accession}' | cat \
        > {%other_acc_list}

#create per-individual information
- action:
    name:  "per_individual_info"
    exec:  "local"
    conda: "main"

    input:
        metadata: "datasets/{%dataset_id}/{=subset_id}/.meta/SraRunTable.txt"

    output:
        indiv_list: "datasets/{%dataset_id}/{=subset_id}/.meta/indiv_list"
        acc2indiv:  "datasets/{%dataset_id}/{=subset_id}/.meta/acc2indiv"
        touch:      "datasets/{%dataset_id}/{=subset_id}/.meta/{%name}.touch"

    shell: |
        #extract list of indiv numbers (at csv column 22 as SAMPLENUMBER_XXX_XXX
        tail -n +2 {%metadata} | mutein cut -f 22 | cut -d_ -f1 | sort -u > {%indiv_list}

        #create file containing only accession and individual
        tail -n +2 {%metadata} | mutein cut -f 1 22 | tr ',_' ' ' | cut -d' ' -f1,2 > {%acc2indiv}

        #create list of accessions from each individual
        for indiv in $(cat {%indiv_list})
        do
            mkdir -p datasets/{%dataset_id}/{=subset_id}/.meta/indivs/${indiv}
            cat {%acc2indiv} | grep -w -e ${indiv} | cut -d' ' -f1 \
                > datasets/{%dataset_id}/{=subset_id}/.meta/indivs/${indiv}/accession_list
        done

        touch {%touch}