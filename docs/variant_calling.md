# Variant Calling

This document walks through the variant calling pipeline using the Keogh2018 dataset as an example. It assumes the processing is taking place on the CS cluster using the skinner compute node which is currently available exclusively for the Mutein project.

## Pipeline to process the Keogh2018 public dataset


### Setting up

Next setup the data folder. Mutein has two large storage areas on the shared HPC file system found under `/SAN/medic/Mutein` (backed up) and `/SAN/medic/MuteinScratch` (not backed up). These are only mounted on demand so you may have to `cd` into them before they show up. Use `df -h` to see how much space is available under each of these storage areas. If you were running your jobs on the HPC gridengine queue from a login node then you would want to store your data on these in order to make it accessible from all compute nodes.

The networked file system is much slower than `skinner`'s local disks, so as we will be using `skinner` for all our compute we'll be using a local disk of `skinner` for the data folder so that it is as fast as possible. `skinner` has two large local disks mounted as `/raid6` (99 terabytes) and `/scratcha` (84 terabytes), you'll need to be setup by an administrator with a subfolder on each of these that you have permission to write to. We will now setup our main data storage folder under `/scratcha` as this is the fastest local disk, and has the best chance of keeping up with demand once we start running multiple local jobs in parallel. Below `USERNAME` should be your username on the HPC:

```
mkdir -p /scratcha/USERNAME/test_keogh2018
cd /scratcha/USERNAME/test_keogh2018
```

We should also create a folder on the shared `/SAN/medic/Mutein` network share where we can store the raw downloaded data so that it will be backed up, such that if the local disks on `skinner` fail we can at least rerun the pipeline without needing to download all the data again:

```
mkdir -p /SAN/medic/Mutein/USERNAME/test_keogh2018
```

Now we need to create a config directory and populate it with settings specific for this run of the pipeline, assuming you have already cloned the repository into `/home/USERNAME/repos/Mutein`. Here I show the use of the `nano` editor (you could also use VS Code Remote SSH extension as previously mentioned) :

```
cd /scratcha/USERNAME/test_keogh2018
mkdir -p config
cp /home/USERNAME/repos/Mutein/mutein/config/mutein_settings_cs ./config/mutein_settings
nano ./config/mutein_settings
```

Now edit the `mutein_settings` file to match the directory names you have used to setup Mutein on your account:

```
export MUT_DIR=/home/USERNAME/repos/Mutein                 #mutein repository
export MUT_DATA=/scratcha/USERNAME/test_keogh2018          #main workspace
export MUT_RAW=/SAN/medic/Mutein/USERNAME/test_keogh2018   #backed up data
```

Use `CTRL-O ENTER` to save once done editing then `CTRL-X` to quit the nano editor.

### Using screen sessions
In order to allow long running jobs to continue without interruption should you encounter network disruption or need to log out of your local computer it is best to run the pipeline inside a `screen` session ([docs](https://www.gnu.org/software/screen/)). Start a new `screen` session called "pipeline":

```
screen -S pipeline
```

Now we can bootstrap the Mutein conda environment to give access to the `yamlmake` command etc:

```
source ./config/mutein_settings
```

You should see a message like:

```
Activating the mutein_main conda environment... OK
Running the mutein script... OK
To begin try:
    yamlmake --help
Or:
    yamlmake --yaml <pipeline> --dry-run
```

For convenience we will make a symlink to the repository folder containing the yamlmake pipeline file:

```
ln -s ${MUT_DIR}/mutein/yamlmake ym
```

For this example we will run the keogh2018 pipeline, therefore use the following command to dryrun the pipeline:

```
yamlmake --yaml ./ym/keogh_pipeline.yml --dry-run
```

Which should run through the pipeline in dry run mode, ie without actually issuing any jobs. It should also create a new folder called `yamlmake_logs` containing the messages generated by the yamlmake run.

To disconnect from the `screen` session use `CTRL-A CTRL-D`. To list available screen sessions use `screen -list`. To reconnect to the session you just created used `screen -R pipeline`.

#### Notes
- use comments to disable includes/modules in the top-level pipeline file
- or else use the --run-until option to stop execution
- need to go through and convert exec: "qsub" to local or parallel
- perhaps use 'exec: "parallel" #qsub or parallel' so we can do a search and replace back and forth between qsub and parallel (or local)
